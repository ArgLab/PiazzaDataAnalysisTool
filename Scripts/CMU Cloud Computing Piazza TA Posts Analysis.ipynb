{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=/Users/lucas/Documents/GCP-service-account/cmu-piazza-nlp.json\n"
     ]
    }
   ],
   "source": [
    "# Need to modify this: get a service account from GCP\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=/Users/lucas/Documents/GCP-service-account/cmu-piazza-nlp.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "from html.parser import HTMLParser\n",
    "from collections import Counter\n",
    "\n",
    "# map: ta_id => filenames\n",
    "def get_ta_to_filename_dict():\n",
    "    filenames = os.listdir('TA_posts')\n",
    "    ta_to_filename = {}\n",
    "    for filename in filenames:\n",
    "        parts_of_filename = filename.split('_')\n",
    "        parts_of_filename[1]\n",
    "        if parts_of_filename[1] not in ta_to_filename:\n",
    "            ta_to_filename[parts_of_filename[1]] = []\n",
    "            ta_to_filename[parts_of_filename[1]].append(filename)\n",
    "        else:\n",
    "            ta_to_filename[parts_of_filename[1]].append(filename)\n",
    "    return ta_to_filename\n",
    "\n",
    "# comparator for filenames\n",
    "def filename_compare(a, b):\n",
    "    start_week_a = int(a.split('_')[4])\n",
    "    start_week_b = int(b.split('_')[4])\n",
    "    return start_week_a - start_week_b\n",
    "\n",
    "# get ta_ids\n",
    "def get_ta_ids(ta_to_filename):\n",
    "    return list(ta_to_filename.keys())\n",
    "\n",
    "# sort the filenames\n",
    "def get_sorted_filenames(ta_id):\n",
    "    return sorted(ta_to_filename[ta_id], key=functools.cmp_to_key(filename_compare))\n",
    "\n",
    "# post data sanitizer\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "# remove html tags\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "# read text, clean the separators and sanitize text\n",
    "def read_ta_posts(filename):\n",
    "    lines = []\n",
    "    with open('TA_posts/' + filename) as f:\n",
    "        for line in f.readlines():\n",
    "            lines.append(line)\n",
    "    cleaned_lines = list(filter(lambda a: a != '----\\n', lines))\n",
    "    sanitized_lines = [strip_tags(line) for line in cleaned_lines]\n",
    "    return sanitized_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USERID_10_Username_week_0_to_3_posts.txt',\n",
       " 'USERID_10_Username_week_3_to_11_posts.txt',\n",
       " 'USERID_10_Username_week_11_to_15_posts.txt',\n",
       " 'USERID_10_Username_week_15_to_18_posts.txt']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example analysis for user 10\n",
    "ta_to_filename = get_ta_to_filename_dict()\n",
    "ta_ids = get_ta_ids(ta_to_filename)\n",
    "user_10_filenames = get_sorted_filenames(ta_ids[0])\n",
    "user_10_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_10_lines_period_1 = read_ta_posts(user_10_filenames[0])\n",
    "user_10_lines_period_2 = read_ta_posts(user_10_filenames[1])\n",
    "user_10_lines_period_3 = read_ta_posts(user_10_filenames[2])\n",
    "user_10_lines_period_4 = read_ta_posts(user_10_filenames[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sentiment analysis\n",
    "# get average sentiment score and magnitude\n",
    "def get_average_sentiment_result(lines):\n",
    "    score = 0\n",
    "    magnitude = 0\n",
    "    for line in lines:\n",
    "        tmp_score, tmp_magnitude = sentiment_text(line)\n",
    "        score += tmp_score\n",
    "        magnitude += tmp_magnitude\n",
    "    return score / len(lines), magnitude / len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -0.10769231101641288\n",
      "Magnitude: 0.569230782871063\n"
     ]
    }
   ],
   "source": [
    "score, magnitude = get_average_sentiment_result(user_10_lines_period_1)\n",
    "print('Score: {}'.format(score))\n",
    "print('Magnitude: {}'.format(magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. syntax analysis\n",
    "# get part of speech structure of user 10's first post in period 1\n",
    "part_of_speech_tag_names, token_contents = syntax_text(user_10_lines_period_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of speech tag: ADP \n",
      "Token content: In\n",
      "\n",
      "Counter({'VERB': 12, 'NOUN': 11, 'PUNCT': 8, 'PRON': 7, 'ADP': 5, 'DET': 5, 'ADV': 1, 'CONJ': 1, 'ADJ': 1})\n",
      "Counter({'it': 3, ',': 2, 'that': 2, 'a': 2, 'security': 2, 'group': 2, 'name': 2, '\"': 2, '.': 2, 'you': 2, 'the': 2, 'In': 1, 'your': 1, 'screenshot': 1, \"'s\": 1, 'complaining': 1, 'could': 1, \"n't\": 1, 'find': 1, 'with': 1, 'SSH_HTTP_MYSQL': 1, 'You': 1, 'should': 1, 'use': 1, 'defined': 1, 'or': 1, 'remove': 1, 'whole': 1, 'parameter': 1, '(': 1, 'so': 1, 'will': 1, 'be': 1, 'using': 1, 'default': 1, 'values': 1, 'like': 1, 'said': 1, ')': 1})\n"
     ]
    }
   ],
   "source": [
    "# show an example mapping\n",
    "print('Part of speech tag: {} \\nToken content: {}\\n'.format(part_of_speech_tag_names[0], token_contents[0]))\n",
    "\n",
    "# check occurrence of tags and tokens in the first post\n",
    "print(Counter(part_of_speech_tag_names))\n",
    "print(Counter(token_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/Computers & Electronics\"\n",
      "confidence: 0.8500000238418579\n",
      "]\n",
      "====================\n",
      "name            : /Computers & Electronics\n",
      "confidence      : 0.8500000238418579\n"
     ]
    }
   ],
   "source": [
    "# 3. content classification analysis\n",
    "# using user 10's first post in peroid 1\n",
    "classify_text(user_10_lines_period_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "name            : security group name\n",
      "type            : OTHER\n",
      "metadata        : {}\n",
      "salience        : 0.4291812777519226\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : screenshot\n",
      "type            : OTHER\n",
      "metadata        : {}\n",
      "salience        : 0.26048579812049866\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : SSH_HTTP_MYSQL\n",
      "type            : OTHER\n",
      "metadata        : {'mid': '/m/0749d', 'wikipedia_url': 'https://en.wikipedia.org/wiki/Secure_Shell'}\n",
      "salience        : 0.15657617151737213\n",
      "wikipedia_url   : https://en.wikipedia.org/wiki/Secure_Shell\n",
      "====================\n",
      "name            : security group\n",
      "type            : ORGANIZATION\n",
      "metadata        : {}\n",
      "salience        : 0.1252780556678772\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : values\n",
      "type            : OTHER\n",
      "metadata        : {}\n",
      "salience        : 0.017576366662979126\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : parameter\n",
      "type            : OTHER\n",
      "metadata        : {}\n",
      "salience        : 0.01090233400464058\n",
      "wikipedia_url   : -\n",
      "Mentions: \n",
      "Name: \"security group name\"\n",
      "  Begin Offset : 121\n",
      "  Content : security group name\n",
      "  Magnitude : 0.800000011920929\n",
      "  Sentiment : 0.800000011920929\n",
      "  Type : 2\n",
      "Salience: 0.4291812777519226\n",
      "Sentiment: magnitude: 1.2000000476837158\n",
      "score: 0.4000000059604645\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"screenshot\"\n",
      "  Begin Offset : 8\n",
      "  Content : screenshot\n",
      "  Magnitude : 0.10000000149011612\n",
      "  Sentiment : -0.10000000149011612\n",
      "  Type : 2\n",
      "Salience: 0.26048579812049866\n",
      "Sentiment: magnitude: 0.10000000149011612\n",
      "score: -0.10000000149011612\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"SSH_HTTP_MYSQL\"\n",
      "  Begin Offset : 87\n",
      "  Content : SSH_HTTP_MYSQL\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 1\n",
      "  Begin Offset : 81\n",
      "  Content : name\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.15657617151737213\n",
      "Sentiment: \n",
      "\n",
      "Mentions: \n",
      "Name: \"security group\"\n",
      "  Begin Offset : 61\n",
      "  Content : security group\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.1252780556678772\n",
      "Sentiment: \n",
      "\n",
      "Mentions: \n",
      "Name: \"values\"\n",
      "  Begin Offset : 222\n",
      "  Content : values\n",
      "  Magnitude : 0.699999988079071\n",
      "  Sentiment : 0.699999988079071\n",
      "  Type : 2\n",
      "Salience: 0.017576366662979126\n",
      "Sentiment: magnitude: 0.699999988079071\n",
      "score: 0.699999988079071\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"parameter\"\n",
      "  Begin Offset : 179\n",
      "  Content : parameter\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.01090233400464058\n",
      "Sentiment: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. entity and entity sentiment analysis\n",
    "# using user 10's first post in peroid 1\n",
    "entities_text(user_10_lines_period_1[0])\n",
    "entity_sentiment_text(user_10_lines_period_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import six\n",
    "\n",
    "# [START language_sentiment_text]\n",
    "def sentiment_text(text):\n",
    "    \"\"\"Detects sentiment in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    # [START language_python_migration_sentiment_text]\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects sentiment in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    sentiment = client.analyze_sentiment(document).document_sentiment\n",
    "    return sentiment.score, sentiment.magnitude\n",
    "    # [END language_python_migration_sentiment_text]\n",
    "# [END language_sentiment_text]\n",
    "\n",
    "\n",
    "# [START language_entities_text]\n",
    "def entities_text(text):\n",
    "    \"\"\"Detects entities in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    # [START language_python_migration_entities_text]\n",
    "    # [START language_python_migration_document_text]\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "    # [END language_python_migration_document_text]\n",
    "\n",
    "    # Detects entities in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    entities = client.analyze_entities(document).entities\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_type = enums.Entity.Type(entity.type)\n",
    "        print('=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', entity.name))\n",
    "        print(u'{:<16}: {}'.format('type', entity_type.name))\n",
    "        print(u'{:<16}: {}'.format('metadata', entity.metadata))\n",
    "        print(u'{:<16}: {}'.format('salience', entity.salience))\n",
    "        print(u'{:<16}: {}'.format('wikipedia_url',\n",
    "              entity.metadata.get('wikipedia_url', '-')))\n",
    "    # [END language_python_migration_entities_text]\n",
    "# [END language_entities_text]\n",
    "\n",
    "# [START language_syntax_text]\n",
    "def syntax_text(text):\n",
    "    \"\"\"Detects syntax in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    # [START language_python_migration_syntax_text]\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects syntax in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    tokens = client.analyze_syntax(document).tokens\n",
    "\n",
    "    part_of_speech_tag_names = []\n",
    "    token_contents = []\n",
    "    for token in tokens:\n",
    "        part_of_speech_tag = enums.PartOfSpeech.Tag(token.part_of_speech.tag)\n",
    "        part_of_speech_tag_names.append(part_of_speech_tag.name)\n",
    "        token_contents.append(token.text.content)\n",
    "    return part_of_speech_tag_names, token_contents\n",
    "    # [END language_python_migration_syntax_text]\n",
    "# [END language_syntax_text]\n",
    "\n",
    "\n",
    "# [START language_entity_sentiment_text]\n",
    "def entity_sentiment_text(text):\n",
    "    \"\"\"Detects entity sentiment in the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    document = types.Document(\n",
    "        content=text.encode('utf-8'),\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detect and send native Python encoding to receive correct word offsets.\n",
    "    encoding = enums.EncodingType.UTF32\n",
    "    if sys.maxunicode == 65535:\n",
    "        encoding = enums.EncodingType.UTF16\n",
    "\n",
    "    result = client.analyze_entity_sentiment(document, encoding)\n",
    "\n",
    "    for entity in result.entities:\n",
    "        print('Mentions: ')\n",
    "        print(u'Name: \"{}\"'.format(entity.name))\n",
    "        for mention in entity.mentions:\n",
    "            print(u'  Begin Offset : {}'.format(mention.text.begin_offset))\n",
    "            print(u'  Content : {}'.format(mention.text.content))\n",
    "            print(u'  Magnitude : {}'.format(mention.sentiment.magnitude))\n",
    "            print(u'  Sentiment : {}'.format(mention.sentiment.score))\n",
    "            print(u'  Type : {}'.format(mention.type))\n",
    "        print(u'Salience: {}'.format(entity.salience))\n",
    "        print(u'Sentiment: {}\\n'.format(entity.sentiment))\n",
    "# [END language_entity_sentiment_text]\n",
    "\n",
    "\n",
    "# [START language_classify_text]\n",
    "def classify_text(text):\n",
    "    \"\"\"Classifies content categories of the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    document = types.Document(\n",
    "        content=text.encode('utf-8'),\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    categories = client.classify_text(document).categories\n",
    "\n",
    "    print(categories)\n",
    "    \n",
    "    for category in categories:\n",
    "        print(u'=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', category.name))\n",
    "        print(u'{:<16}: {}'.format('confidence', category.confidence))\n",
    "# [END language_classify_text]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
