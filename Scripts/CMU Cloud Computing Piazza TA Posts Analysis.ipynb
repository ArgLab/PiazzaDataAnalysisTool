{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=/Users/lucas/Documents/GCP-service-account/cmu-piazza-nlp.json\n"
     ]
    }
   ],
   "source": [
    "# Need to modify this: get a service account from GCP\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=/Users/lucas/Documents/GCP-service-account/cmu-piazza-nlp.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "from html.parser import HTMLParser\n",
    "from collections import Counter\n",
    "\n",
    "# map: ta_id => filenames\n",
    "def get_ta_to_filename_dict():\n",
    "    filenames = os.listdir('TA_posts')\n",
    "    ta_to_filename = {}\n",
    "    for filename in filenames:\n",
    "        parts_of_filename = filename.split('_')\n",
    "        parts_of_filename[1]\n",
    "        if parts_of_filename[1] not in ta_to_filename:\n",
    "            ta_to_filename[parts_of_filename[1]] = []\n",
    "            ta_to_filename[parts_of_filename[1]].append(filename)\n",
    "        else:\n",
    "            ta_to_filename[parts_of_filename[1]].append(filename)\n",
    "    return ta_to_filename\n",
    "\n",
    "# comparator for filenames\n",
    "def filename_compare(a, b):\n",
    "    start_week_a = int(a.split('_')[4])\n",
    "    start_week_b = int(b.split('_')[4])\n",
    "    return start_week_a - start_week_b\n",
    "\n",
    "# get ta_ids\n",
    "def get_ta_ids(ta_to_filename):\n",
    "    return list(ta_to_filename.keys())\n",
    "\n",
    "# sort the filenames\n",
    "def get_sorted_filenames(ta_id):\n",
    "    return sorted(ta_to_filename[ta_id], key=functools.cmp_to_key(filename_compare))\n",
    "\n",
    "# post data sanitizer\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "# remove html tags\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "# read text, clean the separators and sanitize text\n",
    "def read_ta_posts(filename):\n",
    "    lines = []\n",
    "    with open('TA_posts/' + filename) as f:\n",
    "        for line in f.readlines():\n",
    "            lines.append(line)\n",
    "    cleaned_lines = list(filter(lambda a: a != '----\\n', lines))\n",
    "    sanitized_lines = [strip_tags(line) for line in cleaned_lines]\n",
    "    sanitized_non_empty_lines = [line for line in sanitized_lines if line != '\\n']\n",
    "    return sanitized_non_empty_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10': ['USERID_10_Username_week_0_to_3_posts.txt',\n",
       "  'USERID_10_Username_week_3_to_11_posts.txt',\n",
       "  'USERID_10_Username_week_11_to_15_posts.txt',\n",
       "  'USERID_10_Username_week_15_to_18_posts.txt'],\n",
       " '12': ['USERID_12_Username_week_0_to_3_posts.txt',\n",
       "  'USERID_12_Username_week_3_to_18_posts.txt',\n",
       "  'USERID_12_Username_week_12_to_18_posts.txt'],\n",
       " '13': ['USERID_13_Username_week_0_to_5_posts.txt',\n",
       "  'USERID_13_Username_week_5_to_8_posts.txt',\n",
       "  'USERID_13_Username_week_8_to_10_posts.txt',\n",
       "  'USERID_13_Username_week_10_to_18_posts.txt'],\n",
       " '16': ['USERID_16_Username_week_0_to_4_posts.txt',\n",
       "  'USERID_16_Username_week_4_to_9_posts.txt',\n",
       "  'USERID_16_Username_week_9_to_12_posts.txt',\n",
       "  'USERID_16_Username_week_12_to_18_posts.txt'],\n",
       " '2': ['USERID_2_Username_week_0_to_4_posts.txt',\n",
       "  'USERID_2_Username_week_4_to_8_posts.txt',\n",
       "  'USERID_2_Username_week_8_to_18_posts.txt'],\n",
       " '4': ['USERID_4_Username_week_0_to_3_posts.txt',\n",
       "  'USERID_4_Username_week_3_to_6_posts.txt',\n",
       "  'USERID_4_Username_week_6_to_8_posts.txt',\n",
       "  'USERID_4_Username_week_8_to_18_posts.txt'],\n",
       " '9': ['USERID_9_Username_week_0_to_15_posts.txt',\n",
       "  'USERID_9_Username_week_15_to_18_posts.txt']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta_to_filename = get_ta_to_filename_dict()\n",
    "ta_ids = get_ta_ids(ta_to_filename)\n",
    "id_to_filenames = {}\n",
    "for id in ta_ids:\n",
    "    id_to_filenames[id] = get_sorted_filenames(id)\n",
    "id_to_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_posts = {}\n",
    "for id in id_to_filenames:\n",
    "    for filename in id_to_filenames[id]:\n",
    "        if id in id_to_posts:\n",
    "            id_to_posts[id].append(read_ta_posts(filename))\n",
    "        else:\n",
    "            id_to_posts[id] = []\n",
    "            \n",
    "def get_classification_result(user_posts):\n",
    "    count = 0\n",
    "    invalid_post_count = 0\n",
    "    user_classifications = []\n",
    "    for period_posts in user_posts:\n",
    "        for post in period_posts:\n",
    "            try:\n",
    "                tmp = classify_text(post)\n",
    "                if tmp[1] > 0.6:\n",
    "                    tmp.append(post)\n",
    "                    user_classifications.append(tmp)\n",
    "            except:\n",
    "                invalid_post_count += 1\n",
    "            count += 1\n",
    "    return user_classifications, invalid_post_count, count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "for id in id_to_posts:\n",
    "    user_classifications, invalid_count, count = get_classification_result(id_to_posts[id])\n",
    "\n",
    "    with open('ta_' + id + '_post_classification.csv', mode='w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "        csv_writer.writerow(['TA id:', id])\n",
    "        csv_writer.writerow(['Invalid posts count:', invalid_count])\n",
    "        csv_writer.writerow(['Total posts count:', count])\n",
    "        for row in user_classifications:\n",
    "            csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. sentiment analysis\n",
    "# get average sentiment score and magnitude\n",
    "def get_average_sentiment_result(lines):\n",
    "    score = 0\n",
    "    magnitude = 0\n",
    "    for line in lines:\n",
    "        tmp_score, tmp_magnitude = sentiment_text(line)\n",
    "        score += tmp_score\n",
    "        magnitude += tmp_magnitude\n",
    "    return score / len(lines), magnitude / len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -0.10769231101641288\n",
      "Magnitude: 0.569230782871063\n"
     ]
    }
   ],
   "source": [
    "score, magnitude = get_average_sentiment_result(user_10_lines_period_1)\n",
    "print('Score: {}'.format(score))\n",
    "print('Magnitude: {}'.format(magnitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. syntax analysis\n",
    "# get part of speech structure of user 10's first post in period 1\n",
    "part_of_speech_tag_names, token_contents = syntax_text(user_10_lines_period_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of speech tag: ADP \n",
      "Token content: In\n",
      "\n",
      "Counter({'VERB': 12, 'NOUN': 11, 'PUNCT': 8, 'PRON': 7, 'ADP': 5, 'DET': 5, 'ADV': 1, 'CONJ': 1, 'ADJ': 1})\n",
      "Counter({'it': 3, ',': 2, 'that': 2, 'a': 2, 'security': 2, 'group': 2, 'name': 2, '\"': 2, '.': 2, 'you': 2, 'the': 2, 'In': 1, 'your': 1, 'screenshot': 1, \"'s\": 1, 'complaining': 1, 'could': 1, \"n't\": 1, 'find': 1, 'with': 1, 'SSH_HTTP_MYSQL': 1, 'You': 1, 'should': 1, 'use': 1, 'defined': 1, 'or': 1, 'remove': 1, 'whole': 1, 'parameter': 1, '(': 1, 'so': 1, 'will': 1, 'be': 1, 'using': 1, 'default': 1, 'values': 1, 'like': 1, 'said': 1, ')': 1})\n"
     ]
    }
   ],
   "source": [
    "# show an example mapping\n",
    "print('Part of speech tag: {} \\nToken content: {}\\n'.format(part_of_speech_tag_names[0], token_contents[0]))\n",
    "\n",
    "# check occurrence of tags and tokens in the first post\n",
    "print(Counter(part_of_speech_tag_names))\n",
    "print(Counter(token_contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/Computers & Electronics\"\n",
      "confidence: 0.8500000238418579\n",
      "]\n",
      "====================\n",
      "name            : /Computers & Electronics\n",
      "confidence      : 0.8500000238418579\n"
     ]
    }
   ],
   "source": [
    "# 3. content classification analysis\n",
    "# using user 10's first post in peroid 1\n",
    "classify_text(user_10_lines_period_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "name            : security group name\n",
      "type            : OTHER\n",
      "metadata        : {}\n",
      "salience        : 0.4291812777519226\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : screenshot\n",
      "type            : OTHER\n",
      "metadata        : {}\n",
      "salience        : 0.26048579812049866\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : SSH_HTTP_MYSQL\n",
      "type            : OTHER\n",
      "metadata        : {'mid': '/m/0749d', 'wikipedia_url': 'https://en.wikipedia.org/wiki/Secure_Shell'}\n",
      "salience        : 0.15657617151737213\n",
      "wikipedia_url   : https://en.wikipedia.org/wiki/Secure_Shell\n",
      "====================\n",
      "name            : security group\n",
      "type            : ORGANIZATION\n",
      "metadata        : {}\n",
      "salience        : 0.1252780556678772\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : values\n",
      "type            : OTHER\n",
      "metadata        : {}\n",
      "salience        : 0.017576366662979126\n",
      "wikipedia_url   : -\n",
      "====================\n",
      "name            : parameter\n",
      "type            : OTHER\n",
      "metadata        : {}\n",
      "salience        : 0.01090233400464058\n",
      "wikipedia_url   : -\n",
      "Mentions: \n",
      "Name: \"security group name\"\n",
      "  Begin Offset : 121\n",
      "  Content : security group name\n",
      "  Magnitude : 0.800000011920929\n",
      "  Sentiment : 0.800000011920929\n",
      "  Type : 2\n",
      "Salience: 0.4291812777519226\n",
      "Sentiment: magnitude: 1.2000000476837158\n",
      "score: 0.4000000059604645\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"screenshot\"\n",
      "  Begin Offset : 8\n",
      "  Content : screenshot\n",
      "  Magnitude : 0.10000000149011612\n",
      "  Sentiment : -0.10000000149011612\n",
      "  Type : 2\n",
      "Salience: 0.26048579812049866\n",
      "Sentiment: magnitude: 0.10000000149011612\n",
      "score: -0.10000000149011612\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"SSH_HTTP_MYSQL\"\n",
      "  Begin Offset : 87\n",
      "  Content : SSH_HTTP_MYSQL\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 1\n",
      "  Begin Offset : 81\n",
      "  Content : name\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.15657617151737213\n",
      "Sentiment: \n",
      "\n",
      "Mentions: \n",
      "Name: \"security group\"\n",
      "  Begin Offset : 61\n",
      "  Content : security group\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.1252780556678772\n",
      "Sentiment: \n",
      "\n",
      "Mentions: \n",
      "Name: \"values\"\n",
      "  Begin Offset : 222\n",
      "  Content : values\n",
      "  Magnitude : 0.699999988079071\n",
      "  Sentiment : 0.699999988079071\n",
      "  Type : 2\n",
      "Salience: 0.017576366662979126\n",
      "Sentiment: magnitude: 0.699999988079071\n",
      "score: 0.699999988079071\n",
      "\n",
      "\n",
      "Mentions: \n",
      "Name: \"parameter\"\n",
      "  Begin Offset : 179\n",
      "  Content : parameter\n",
      "  Magnitude : 0.0\n",
      "  Sentiment : 0.0\n",
      "  Type : 2\n",
      "Salience: 0.01090233400464058\n",
      "Sentiment: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. entity and entity sentiment analysis\n",
    "# using user 10's first post in peroid 1\n",
    "entities_text(user_10_lines_period_1[0])\n",
    "entity_sentiment_text(user_10_lines_period_1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "import six\n",
    "\n",
    "# [START language_sentiment_text]\n",
    "def sentiment_text(text):\n",
    "    \"\"\"Detects sentiment in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    # [START language_python_migration_sentiment_text]\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects sentiment in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    sentiment = client.analyze_sentiment(document).document_sentiment\n",
    "    return sentiment.score, sentiment.magnitude\n",
    "    # [END language_python_migration_sentiment_text]\n",
    "# [END language_sentiment_text]\n",
    "\n",
    "\n",
    "# [START language_entities_text]\n",
    "def entities_text(text):\n",
    "    \"\"\"Detects entities in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    # [START language_python_migration_entities_text]\n",
    "    # [START language_python_migration_document_text]\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "    # [END language_python_migration_document_text]\n",
    "\n",
    "    # Detects entities in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    entities = client.analyze_entities(document).entities\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_type = enums.Entity.Type(entity.type)\n",
    "        print('=' * 20)\n",
    "        print(u'{:<16}: {}'.format('name', entity.name))\n",
    "        print(u'{:<16}: {}'.format('type', entity_type.name))\n",
    "        print(u'{:<16}: {}'.format('metadata', entity.metadata))\n",
    "        print(u'{:<16}: {}'.format('salience', entity.salience))\n",
    "        print(u'{:<16}: {}'.format('wikipedia_url',\n",
    "              entity.metadata.get('wikipedia_url', '-')))\n",
    "    # [END language_python_migration_entities_text]\n",
    "# [END language_entities_text]\n",
    "\n",
    "# [START language_syntax_text]\n",
    "def syntax_text(text):\n",
    "    \"\"\"Detects syntax in the text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    # Instantiates a plain text document.\n",
    "    # [START language_python_migration_syntax_text]\n",
    "    document = types.Document(\n",
    "        content=text,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detects syntax in the document. You can also analyze HTML with:\n",
    "    #   document.type == enums.Document.Type.HTML\n",
    "    tokens = client.analyze_syntax(document).tokens\n",
    "\n",
    "    part_of_speech_tag_names = []\n",
    "    token_contents = []\n",
    "    for token in tokens:\n",
    "        part_of_speech_tag = enums.PartOfSpeech.Tag(token.part_of_speech.tag)\n",
    "        part_of_speech_tag_names.append(part_of_speech_tag.name)\n",
    "        token_contents.append(token.text.content)\n",
    "    return part_of_speech_tag_names, token_contents\n",
    "    # [END language_python_migration_syntax_text]\n",
    "# [END language_syntax_text]\n",
    "\n",
    "\n",
    "# [START language_entity_sentiment_text]\n",
    "def entity_sentiment_text(text):\n",
    "    \"\"\"Detects entity sentiment in the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    document = types.Document(\n",
    "        content=text.encode('utf-8'),\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    # Detect and send native Python encoding to receive correct word offsets.\n",
    "    encoding = enums.EncodingType.UTF32\n",
    "    if sys.maxunicode == 65535:\n",
    "        encoding = enums.EncodingType.UTF16\n",
    "\n",
    "    result = client.analyze_entity_sentiment(document, encoding)\n",
    "\n",
    "    for entity in result.entities:\n",
    "        print('Mentions: ')\n",
    "        print(u'Name: \"{}\"'.format(entity.name))\n",
    "        for mention in entity.mentions:\n",
    "            print(u'  Begin Offset : {}'.format(mention.text.begin_offset))\n",
    "            print(u'  Content : {}'.format(mention.text.content))\n",
    "            print(u'  Magnitude : {}'.format(mention.sentiment.magnitude))\n",
    "            print(u'  Sentiment : {}'.format(mention.sentiment.score))\n",
    "            print(u'  Type : {}'.format(mention.type))\n",
    "        print(u'Salience: {}'.format(entity.salience))\n",
    "        print(u'Sentiment: {}\\n'.format(entity.sentiment))\n",
    "# [END language_entity_sentiment_text]\n",
    "\n",
    "\n",
    "# [START language_classify_text]\n",
    "def classify_text(text):\n",
    "    \"\"\"Classifies content categories of the provided text.\"\"\"\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode('utf-8')\n",
    "\n",
    "    document = types.Document(\n",
    "        content=text.encode('utf-8'),\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "\n",
    "    categories = client.classify_text(document).categories\n",
    "\n",
    "#     print(categories)\n",
    "    \n",
    "#     for category in categories:\n",
    "#         print(u'=' * 20)\n",
    "#         print(u'{:<16}: {}'.format('name', category.name))\n",
    "#         print(u'{:<16}: {}'.format('confidence', category.confidence))\n",
    "    return [category.name, category.confidence]\n",
    "# [END language_classify_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In your screenshot, it\\'s complaining that it couldn\\'t find a security group with name \"SSH_HTTP_MYSQL\". You should use a security group name that you defined, or remove the whole parameter (so it will be using the default\\xa0values like you said).\\n',\n",
       " 'Are you saying the\\xa0\"--security-groups\" parameter for the run-instance command? If so, you may have to read the manual again since what you provided is\\xa0not in a correct format.\\xa0https://docs.aws.amazon.com/cli/latest/reference/ec2/run-instances.html\\n',\n",
       " \"No, it doesn't, you can install sysbench by running `apt-get update` then `apt-get install sysbench`. For higher version ones, please find its corresponding parameter for init-rng.\\n\",\n",
       " 'Did you pick the basic Ubuntu AMI? It seems that the version of your sysbench is higher than the default one installed by apt-get.\\n',\n",
       " 'If you start\\xa0a new instance (ami-41e0b93b) with the 2 lines of command you listed above, it should work.\\xa0Are you saying that you are still getting \"The virtual environment was not created successfully because ensurepip is not available\" error after you install python3-venv? Did you get any error\\xa0while installing python3-venv?\\n',\n",
       " 'One possible reason is that your\\xa0terminal automatically sets wrong environment variables (LC_ALL, LC_CTYPE).\\xa0https://askubuntu.com/questions/599808/cannot-set-lc-ctype-to-default-locale-no-such-file-or-directory\\n',\n",
       " 'If you are able to ssh into your machine which means the external IP works. You may have to check if you allow http (port 80 is open) for that instance.\\n',\n",
       " 'Which VM are you saying here? Are you\\xa0executing runner.sh on the aws\\xa0instance (ami-dd87aca7)?\\n',\n",
       " 'If you \"terminate\" the instance and it won\\'t continue to charge you, which also means that your data stored in EBS will be gone.\\n',\n",
       " 'Hi guys,\\xa0this is a reminder that we do have office hours held by several TAs at SV, please\\xa0feel free\\xa0to come and discuss any question you have! Mine\\xa0is\\xa0at B18 lobby area from\\xa03:30pm to 5:30pm on Thursday. Other ones can be found here:\\xa0https://piazza.com/cmu/spring2018/1531915619/staff\\xa0.\\n',\n",
       " 'Testing your code locally or one small dataset will help you to find the issue. You can try different corner cases and see if they are all covered. For example, did you set the threshold on view count?\\n',\n",
       " 'No, all the requirements are listed in the write-up. You\\xa0may also want to check if your logic is correct or not for handling the last title in reducer.\\n',\n",
       " 'Try to locate the problem in log file first. Log files can be found under each attempt folder. Refer to\\xa0\"Troubleshooting EMR\" section in the\\xa0write-up.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_10_lines_period_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1, similarity between 'that' and 'it': -0.03197614\n",
      "Model 2, similarity between 'that' and 'it': -0.0058905706\n"
     ]
    }
   ],
   "source": [
    "# reference: https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "\n",
    "import gensim \n",
    "from gensim.models import Word2Vec \n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "# iterate through each sentence in the list \n",
    "for line in user_10_lines_period_1:\n",
    "    for i in sent_tokenize(line): \n",
    "        temp = [] \n",
    "        # tokenize the sentence into words \n",
    "        for j in word_tokenize(i): \n",
    "            temp.append(j.lower()) \n",
    "        data.append(temp) \n",
    "\n",
    "# Create CBOW model \n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5) \n",
    "print(\"Model 1, similarity between 'that' and 'it': \" + str(model1.similarity('that', 'it')))\n",
    "\n",
    "# Create Skip Gram model \n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5, sg = 1) \n",
    "print(\"Model 2, similarity between 'that' and 'it': \" + str(model2.similarity('that', 'it')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
